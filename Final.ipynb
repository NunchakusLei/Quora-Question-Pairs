{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python -W ignore::DeprecationWarning\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "import seaborn as sns\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import os\n",
    "\n",
    "import gensim\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.similarities import WmdSimilarity\n",
    "from gensim.models import KeyedVectors\n",
    "if not os.path.exists('data/GoogleNews-vectors-negative300.bin.gz'):\n",
    "    raise ValueError(\"SKIP: You need to download the google news model\")\n",
    "from gensim.models import Doc2Vec\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "from sner import Ner\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag.stanford import CoreNLPPOSTagger \n",
    "\n",
    "# Change the path according to your system\n",
    "stanford_classifier = 'http://localhost:9199'\n",
    "stanford_pos_tagger = 'http://localhost:9000'\n",
    "NERst = Ner(host='localhost',port=9199)\n",
    "\n",
    "from pyemd import emd \n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stop_words = stopwords.words('english')\n",
    "_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = clean_text_df(df)\n",
    "    # df = stem(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(df):\n",
    "    \"\"\"\n",
    "    Process the text data with SnowballStemmer\n",
    "    :param df: dataframe of original data\n",
    "    :return: dataframe after stemming\n",
    "    \"\"\"\n",
    "    df['question1'] = df.question1.map(lambda x: ' '.join(\n",
    "        [_stemmer.stem(word) for word in\n",
    "         nltk.word_tokenize(clean_text(str(x).lower()))]))\n",
    "    df['question2'] = df.question2.map(lambda x: ' '.join(\n",
    "        [_stemmer.stem(word) for word in\n",
    "         nltk.word_tokenize(clean_text(str(x).lower()))]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_df(df):\n",
    "    \"\"\"\n",
    "    Process the text data with SnowballStemmer\n",
    "    :param df: dataframe of original data\n",
    "    :return: dataframe after stemming\n",
    "    \"\"\"\n",
    "    df['question1'] = df.question1.map(lambda x: clean_text(str(x).lower()))\n",
    "    df['question2'] = df.question2.map(lambda x: clean_text(str(x).lower()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    \"\"\"\n",
    "    Clean text\n",
    "    :param text: the string of text\n",
    "    :return: text string after cleaning\n",
    "    \"\"\"\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\").replace(\"\\“\", \"\\\"\").replace(\"\\”\", \"\\\"\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\" fb \", \" facebook \")\\\n",
    "                           .replace(\"facebooks\", \" facebook \").replace(\"insidefacebook\", \"inside facebook\")\\\n",
    "                           .replace(\"=\", \" equal \").replace(\"pokemons\", \"pokemon\").replace(\"pokémon\", \"pokemon\")\n",
    "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "    \n",
    "    # remove extra space\n",
    "    x = ' '.join(x.split())\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "train_file_name = \"train\";\n",
    "test_file_name = \"test\";\n",
    "train_data = pd.read_csv(path+train_file_name+'.csv')\n",
    "test_data = pd.read_csv(path+test_file_name+'.csv')\n",
    "\n",
    "# train_data = pd.read_csv(path+'quora_train_features.csv',encoding = \"utf-8\")\n",
    "# test_data = pd.read_csv(path+'quora_test_features.csv',encoding = \"utf-8\")\n",
    "train_data = pd.DataFrame(train_data)\n",
    "test_data = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(train_data)\n",
    "test_data = preprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s1 = [w for w in s1 if w.isalpha()]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w.isalpha()]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s1 = [w for w in s1 if w.isalpha()]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w.isalpha()]\n",
    "    return norm_model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(data, norm):\n",
    "    if (norm):\n",
    "        norm_model.init_sims(replace=True)\n",
    "        data['norm_wmd'] = data.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
    "    else:\n",
    "        data['w2v'] = data.apply(lambda x: wmd(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.codeproject.com/Articles/11835/WordNet-based-semantic-similarity-measurement\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for syn1 in synsets1:\n",
    "        arr_simi_score = []\n",
    "        for syn2 in synsets2:\n",
    "            simi_score = syn1.path_similarity(syn2)\n",
    "            if simi_score is not None:\n",
    "                arr_simi_score.append(simi_score)\n",
    "            else:\n",
    "                arr_simi_score.append(0)\n",
    "        if(len(arr_simi_score) > 0):\n",
    "            best = max(arr_simi_score)\n",
    "            score += best\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        score /= count\n",
    "    return score\n",
    "\n",
    "def symmetric_sentence_similarity(index,sentence1, sentence2):\n",
    "    if( len(sentence1)==0 or len(sentence2) == 0):\n",
    "            return 0\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_similarity(sentence1, sentence2) + sentence_similarity(sentence2, sentence1)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_sentence_similarity(index, sentence1, sentence2):\n",
    "    if( int(index)%100000 == 0):\n",
    "        print(time.strftime(\"%c\"))\n",
    "        print(index)\n",
    "    return nlp(sentence1).similarity(nlp(sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(data):\n",
    "    data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
    "    data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
    "    data['diff_len'] = data.len_q1 - data.len_q2\n",
    "    data['len_unique_char_q1'] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    data['len_unique_char_q2'] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "    data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
    "    data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "    data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "#    data['fuzz_partial_token_set_ratio'] = data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_2(data):\n",
    "    data['semantic_similarity'] = data.apply(lambda x: semantic_sentence_similarity( x.name,str(x['question1']), str(x['question2'])) , axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_3(data):\n",
    "    question1_vectors = np.zeros((data.shape[0], 300))\n",
    "    error_count = 0\n",
    "    for i, q in tqdm(enumerate(data.question1.values)):\n",
    "        question1_vectors[i, :] = sent2vec(q)\n",
    "    question2_vectors  = np.zeros((data.shape[0], 300))\n",
    "    for i, q in tqdm(enumerate(data.question2.values)):\n",
    "        question2_vectors[i, :] = sent2vec(q)\n",
    "    data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "    data['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "    data['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "    data['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]\n",
    "    cPickle.dump(question1_vectors, open('data/q1_w2v.pkl', 'wb'), -1)\n",
    "    cPickle.dump(question2_vectors, open('data/q2_w2v.pkl', 'wb'), -1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_4(data):\n",
    "    data['sentence_similarity'] = data.apply(lambda x: symmetric_sentence_similarity(x.name, str(x['question1']), str(x['question2'])), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "ques = pd.concat([train_data[['question1', 'question2']],\n",
    "                  test_data[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "ques.shape\n",
    "\n",
    "def q1_freq(row):\n",
    "    return (len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq(row):\n",
    "    return (len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "def feature_5(df):\n",
    "    for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "    df['q1_q2_intersect'] = df.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "    df['q1_freq'] = df.apply(q1_freq, axis=1, raw=True)\n",
    "    df['q2_freq'] = df.apply(q2_freq, axis=1, raw=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_match_share(q1, q2, stop_words=None):\n",
    "    q1 = str(q1).lower().split()\n",
    "    q2 = str(q2).lower().split()\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in q1:\n",
    "        if word not in stop_words:\n",
    "            q1words[word] = 1\n",
    "    for word in q2:\n",
    "        if word not in stop_words:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0.\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_6(row):\n",
    "    for i in range(ques.shape[0]):\n",
    "        wm = word_match_share(ques.question1[i], ques.question2[i], stop_words=stop_words)\n",
    "        q_dict[ques.question1[i]][ques.question2[i]] = wm\n",
    "        q_dict[ques.question2[i]][ques.question1[i]] = wm\n",
    "        q1 = q_dict[row['question1']]\n",
    "        q2 = q_dict[row['question2']]\n",
    "    inter_keys = set(q1.keys()).intersection(set(q2.keys()))\n",
    "    if(len(inter_keys) == 0): return 0.\n",
    "    inter_wm = 0.\n",
    "    total_wm = 0.\n",
    "    for q,wm in q1.items():\n",
    "        if q in inter_keys:\n",
    "            inter_wm += wm\n",
    "        total_wm += wm\n",
    "    for q,wm in q2.items():\n",
    "        if q in inter_keys:\n",
    "            inter_wm += wm\n",
    "        total_wm += wm\n",
    "    if(total_wm == 0.): return 0.\n",
    "    return inter_wm/total_wm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance\n",
    "SAFE_DIV = 0.0001\n",
    "# also pre-processing: tokennizing questions, removing empty q's, seperating stop words， then find all features\n",
    "def get_token_features(q1, q2):\n",
    "    token_features = [0.0]*10\n",
    "\n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    q1_words = set([word for word in q1_tokens if word not in stop_words])\n",
    "    q2_words = set([word for word in q2_tokens if word not in stop_words])\n",
    "\n",
    "    q1_stops = set([word for word in q1_tokens if word in stop_words])\n",
    "    q2_stops = set([word for word in q2_tokens if word in stop_words])\n",
    "\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "\n",
    "# feature extraction\n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)  # check ratio between common and smaller(processed)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)  # check ratio between common and larger(processed)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)  # check ratio between common and smaller(stop words)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)  # check ratio between common and larger(stop words)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV) # check ratio between common and smaller(total)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV) # check ratio between common and larger(total)\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1]) # check if endings are the same\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0]) # check if beginnings are the same\n",
    "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens)) # check diff between line\n",
    "    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2 # check average length\n",
    "    return token_features\n",
    "\n",
    "\n",
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a, b))\n",
    "    if len(strs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
    "    \n",
    "def feature_7(data):\n",
    "    token_features = data.apply(lambda x: get_token_features(str(x[\"question1\"]), str(x[\"question2\"])), axis=1)\n",
    "    data[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "    data[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "    data[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "    data[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "    data[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "    data[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "    data[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "    data[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
    "    data[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
    "    data[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
    "    data[\"longest_substr_ratio\"]  = data.apply(lambda x: get_longest_substr_ratio(str(x[\"question1\"]), str(x[\"question2\"])), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "NB_CORES = 10\n",
    "FREQ_UPPER_BOUND = 100\n",
    "NEIGHBOR_UPPER_BOUND = 5\n",
    "\n",
    "\n",
    "def create_question_hash(train_df, test_df):\n",
    "    train_qs = np.dstack([train_df[\"question1\"], train_df[\"question2\"]]).flatten() # adds all training questions into one list\n",
    "    test_qs = np.dstack([test_df[\"question1\"], test_df[\"question2\"]]).flatten() # adds all testing questions into one list\n",
    "    all_qs = np.append(train_qs, test_qs) # all questions together\n",
    "    all_qs = pd.DataFrame(all_qs)[0].drop_duplicates() # all unique questions\n",
    "    all_qs.reset_index(inplace=True, drop=True) # create a new table with all questions\n",
    "    question_dict = pd.Series(all_qs.index.values, index=all_qs.values).to_dict()\n",
    "    return question_dict\n",
    "\n",
    "\n",
    "def get_hash(df, hash_dict):\n",
    "    df[\"qid1\"] = df[\"question1\"].map(hash_dict)\n",
    "    df[\"qid2\"] = df[\"question2\"].map(hash_dict)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_kcore_dict(df):\n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(df.qid1)\n",
    "    edges = list(df[[\"qid1\", \"qid2\"]].to_records(index=False))\n",
    "    g.add_edges_from(edges)\n",
    "    g.remove_edges_from(g.selfloop_edges())\n",
    "\n",
    "    df_output = pd.DataFrame(data=list(g.nodes()), columns=[\"qid\"])\n",
    "    df_output[\"kcore\"] = 0\n",
    "    for k in range(2, NB_CORES + 1):\n",
    "        ck = nx.k_core(g, k=k).nodes()\n",
    "        print(\"kcore\", k)\n",
    "        df_output.ix[df_output.qid.isin(ck), \"kcore\"] = k\n",
    "\n",
    "    return df_output.to_dict()[\"kcore\"]\n",
    "\n",
    "\n",
    "def get_kcore_features(df, kcore_dict):\n",
    "    df[\"kcore1\"] = df[\"qid1\"].apply(lambda x: kcore_dict[x])\n",
    "    df[\"kcore2\"] = df[\"qid2\"].apply(lambda x: kcore_dict[x])\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_minmax(df, col):\n",
    "    sorted_features = np.sort(np.vstack([df[col + \"1\"], df[col + \"2\"]]).T)\n",
    "    df[\"min_\" + col] = sorted_features[:, 0]\n",
    "    df[\"max_\" + col] = sorted_features[:, 1]\n",
    "    return df.drop([col + \"1\", col + \"2\"], axis=1)\n",
    "\n",
    "\n",
    "def get_neighbors(train_df, test_df):\n",
    "    neighbors = defaultdict(set)\n",
    "    for df in [train_df, test_df]:\n",
    "        for q1, q2 in zip(df[\"qid1\"], df[\"qid2\"]):\n",
    "            neighbors[q1].add(q2)\n",
    "            neighbors[q2].add(q1)\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def get_neighbor_features(df, neighbors):\n",
    "    common_nc = df.apply(lambda x: len(neighbors[x.qid1].intersection(neighbors[x.qid2])), axis=1)\n",
    "    min_nc = df.apply(lambda x: min(len(neighbors[x.qid1]), len(neighbors[x.qid2])), axis=1)\n",
    "    df[\"common_neighbor_ratio\"] = common_nc / min_nc\n",
    "    df[\"common_neighbor_count\"] = common_nc.apply(lambda x: min(x, NEIGHBOR_UPPER_BOUND))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_freq_features(df, frequency_map):\n",
    "    df[\"freq1\"] = df[\"qid1\"].map(lambda x: min(frequency_map[x], FREQ_UPPER_BOUND))\n",
    "    df[\"freq2\"] = df[\"qid2\"].map(lambda x: min(frequency_map[x], FREQ_UPPER_BOUND))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = feature(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = feature(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 10 00:21:08 2017\n",
      "0\n",
      "Sun Dec 10 00:51:50 2017\n",
      "100000\n",
      "Sun Dec 10 01:22:15 2017\n",
      "200000\n",
      "Sun Dec 10 01:52:29 2017\n",
      "300000\n",
      "Sun Dec 10 02:22:42 2017\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "train_data = feature_2(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 10 02:24:01 2017\n",
      "0\n",
      "Sun Dec 10 02:54:16 2017\n",
      "100000\n",
      "Sun Dec 10 03:24:33 2017\n",
      "200000\n",
      "Sun Dec 10 03:54:36 2017\n",
      "300000\n",
      "Sun Dec 10 04:24:48 2017\n",
      "400000\n",
      "Sun Dec 10 04:55:02 2017\n",
      "500000\n",
      "Sun Dec 10 05:25:17 2017\n",
      "600000\n",
      "Sun Dec 10 05:55:34 2017\n",
      "700000\n",
      "Sun Dec 10 06:25:51 2017\n",
      "800000\n",
      "Sun Dec 10 06:56:10 2017\n",
      "900000\n",
      "Sun Dec 10 07:26:27 2017\n",
      "1000000\n",
      "Sun Dec 10 07:56:46 2017\n",
      "1100000\n",
      "Sun Dec 10 08:27:04 2017\n",
      "1200000\n",
      "Sun Dec 10 08:57:23 2017\n",
      "1300000\n",
      "Sun Dec 10 09:27:43 2017\n",
      "1400000\n",
      "Sun Dec 10 09:58:03 2017\n",
      "1500000\n",
      "Sun Dec 10 10:28:22 2017\n",
      "1600000\n",
      "Sun Dec 10 10:58:42 2017\n",
      "1700000\n",
      "Sun Dec 10 11:29:01 2017\n",
      "1800000\n",
      "Sun Dec 10 11:59:20 2017\n",
      "1900000\n",
      "Sun Dec 10 12:29:46 2017\n",
      "2000000\n",
      "Sun Dec 10 13:00:44 2017\n",
      "2100000\n",
      "Sun Dec 10 13:31:42 2017\n",
      "2200000\n",
      "Sun Dec 10 14:02:43 2017\n",
      "2300000\n"
     ]
    }
   ],
   "source": [
    "test_data = feature_2(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('data/glove.840B.300d.txt.word2vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format('data/glove.840B.300d.txt.word2vec', binary=False)\n",
    "norm_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = w2v(train_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = w2v(train_data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = w2v(test_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = w2v(test_data, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "404290it [01:01, 6607.25it/s]\n",
      "404290it [01:02, 6492.79it/s]\n",
      "c:\\program files\\python36\\lib\\site-packages\\scipy\\spatial\\distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "c:\\program files\\python36\\lib\\site-packages\\scipy\\spatial\\distance.py:763: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = np.double(unequal_nonzero.sum()) / np.double(nonzero.sum())\n",
      "c:\\program files\\python36\\lib\\site-packages\\scipy\\spatial\\distance.py:980: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return l1_diff.sum() / l1_sum.sum()\n"
     ]
    }
   ],
   "source": [
    "train_data = feature_3(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "2345796it [06:03, 6455.75it/s]\n",
      "2345796it [06:12, 6292.99it/s]\n",
      "c:\\program files\\python36\\lib\\site-packages\\scipy\\spatial\\distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "c:\\program files\\python36\\lib\\site-packages\\scipy\\spatial\\distance.py:763: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = np.double(unequal_nonzero.sum()) / np.double(nonzero.sum())\n",
      "c:\\program files\\python36\\lib\\site-packages\\scipy\\spatial\\distance.py:980: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return l1_diff.sum() / l1_sum.sum()\n"
     ]
    }
   ],
   "source": [
    "test_data = feature_3(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = feature_4(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = feature_4(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dict = defaultdict(set)\n",
    "\n",
    "train_data= feature_5(train_data)\n",
    "test_data= feature_5(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_dict = defaultdict(dict)\n",
    "# train_data['q1_q2_wm_ratio'] = train_data.apply(feature_6, axis=1, raw=True)\n",
    "# test_data['q1_q2_wm_ratio'] = test_data.apply(feature_6, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hashing the questions...\")\n",
    "question_dict = create_question_hash(train_data, test_data)\n",
    "train_data = get_hash(train_data, question_dict)\n",
    "test_data = get_hash(test_data, question_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating kcore features...\")\n",
    "all_df = pd.concat([train_data, test_data])\n",
    "kcore_dict = get_kcore_dict(all_df)\n",
    "train_data = get_kcore_features(train_data, kcore_dict)\n",
    "test_data = get_kcore_features(test_data, kcore_dict)\n",
    "train_data = convert_to_minmax(train_data, \"kcore\")\n",
    "test_data = convert_to_minmax(test_data, \"kcore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating common neighbor features...\")\n",
    "neighbors = get_neighbors(train_data, test_data)\n",
    "train_data = get_neighbor_features(train_data, neighbors)\n",
    "test_data = get_neighbor_features(test_data, neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating frequency features...\")\n",
    "frequency_map = dict(zip(*np.unique(np.vstack((all_df[\"qid1\"], all_df[\"qid2\"])), return_counts=True)))\n",
    "train_data = get_freq_features(train_data, frequency_map)\n",
    "test_data = get_freq_features(test_data, frequency_map)\n",
    "train_data = convert_to_minmax(train_data, \"freq\")\n",
    "test_data = convert_to_minmax(test_data, \"freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = feature_7(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = feature_7(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv', \n",
    "                       dtype={\n",
    "                           'question1': np.str,\n",
    "                           'question2': np.str\n",
    "                       })\n",
    "df_test = pd.read_csv('data/test.csv', \n",
    "                      dtype={\n",
    "                          'question1': np.str,\n",
    "                          'question2': np.str\n",
    "                      })\n",
    "df_train = df_train.fillna(' ')\n",
    "df_test = df_test.fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = set(df_train['question1']) | set(df_train['question2']) | set(df_test['question1']) | set(df_test['question2'])\n",
    "qs = list(qs)\n",
    "q2id = dict(zip(qs, range(len(qs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['qid1'] = df_train['question1'].apply(q2id.get)\n",
    "df_train['qid2'] = df_train['question2'].apply(q2id.get)\n",
    "df_test['qid1'] = df_test['question1'].apply(q2id.get)\n",
    "df_test['qid2'] = df_test['question2'].apply(q2id.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "m = coo_matrix((np.ones(2 * (df_train.shape[0] + df_test.shape[0])),\n",
    "                (pd.concat((df_train['qid1'], df_test['qid1'], df_train['qid2'], df_test['qid2']), axis=0).values,\n",
    "                 pd.concat((df_train['qid2'], df_test['qid2'], df_train['qid1'], df_test['qid1']), axis=0).values)),\n",
    "               shape=(len(qs), len(qs)))\n",
    "m = csr_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def cosine_similarities(mat):\n",
    "    col_normed_mat = normalize(mat.tocsc(), axis=0)\n",
    "    return col_normed_mat.T * col_normed_mat\n",
    "\n",
    "def jaccard_similarities(mat):\n",
    "    cols_sum = mat.getnnz(axis=0)\n",
    "    ab = mat.T * mat\n",
    "\n",
    "    # for rows\n",
    "    aa = np.repeat(cols_sum, ab.getnnz(axis=0))\n",
    "    # for columns\n",
    "    bb = cols_sum[ab.indices]\n",
    "\n",
    "    similarities = ab.copy()\n",
    "    similarities.data /= (aa + bb - ab.data)\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m_cos = cosine_similarities(m)\n",
    "m_jac = jaccard_similarities(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cos = m_cos.todok()\n",
    "m_jac = m_jac.todok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def devil_cos(row):\n",
    "    return m_cos[row['qid1'], row['qid2']]\n",
    "\n",
    "def devil_jaccard(row):\n",
    "    return m_jac[row['qid1'], row['qid2']]\n",
    "\n",
    "df_train['devil_cos'] = df_train.apply(devil_cos, axis=1, raw=True)\n",
    "df_train['devil_jaccard'] = df_train.apply(devil_jaccard, axis=1, raw=True)\n",
    "df_test['devil_cos'] = df_test.apply(devil_cos, axis=1, raw=True)\n",
    "df_test['devil_jaccard'] = df_test.apply(devil_jaccard, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['id','qid1','qid2','question1','question2','is_duplicate'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['test_id','qid1','qid2','question1','question2'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_data,df_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([test_data,df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(path+'quora_train_features.csv', index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(path+'quora_test_features.csv', index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleared_train = pd.read_csv(path+'quora_train_features.csv')\n",
    "# cleared_test = pd.read_csv(path+'quora_train_features.csv')\n",
    "# cleared_train = pd.read_csv(path+'quora_train_features_cleared.csv')\n",
    "# cleared_test = pd.read_csv(path+'quora_test_features_cleared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_train = train_data\n",
    "cleared_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleared_train = cleared_train.drop(['id','qid1','qid2','question1','question2','is_duplicate'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleared_test = cleared_test.drop(['test_id','qid1','qid2','question1','question2'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleared_test.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_train.to_csv('data/quora_train_features_cleared.csv', index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_test.to_csv('data/quora_test_features_cleared.csv', index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleared_train = pd.read_csv(path+'quora_train_features_cleared_best.csv')\n",
    "# cleared_test = pd.read_csv(path+'quora_test_features_cleared_best.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleared_train = cleared_train.drop(['q1_q2_wm_ratio','common_neighbor_ratio'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleared_test = cleared_test.drop(['q1_q2_wm_ratio','common_neighbor_ratio'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleared_train = cleared_train.drop(['fuzz_partial_token_set_ratio'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleared_test = cleared_test.drop(['fuzz_partial_token_set_ratio'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = train_data['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_train = cleared_train.replace(np.nan, 0.)\n",
    "cleared_train = cleared_train.replace(np.inf, 99.)\n",
    "cleared_test = cleared_test.replace(np.nan, 0.)\n",
    "cleared_test = cleared_test.replace(np.inf, 99.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(cleared_train, ground_truth, test_size=0.1, random_state=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 7\n",
    "params['subsample'] = 0.75\n",
    "params['base_score'] = 0.2\n",
    "\n",
    "params['colsample_bytree'] = 1\n",
    "params['colsample_bylevel'] = 1\n",
    "params['n_jobs'] = -1\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(cleared_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bst = xgb.train(params, d_train, 5000, watchlist, early_stopping_rounds=100, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = bst.predict(d_test)\n",
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.165 / 0.37\n",
    "b = (1 - 0.165) / (1 - 0.37)\n",
    "\n",
    "def fix_prob(x):\n",
    "    return a * x / (a * x + b * (1 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = test_data.index.values\n",
    "sub['is_duplicate'] = fix_prob(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('data/simple_xgb2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_train =  pd.read_csv('data/train.csv')\n",
    "df_test =  pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPLACE it with:\n",
    "test_label = np.array(pd.read_csv('data/simple_xgb2_test.csv')[\"is_duplicate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "REPEAT = 2 #a reasonable number which can consider your updates iteratively but not ruin the predictions\n",
    "\n",
    "DUP_THRESHOLD = 0.5 #classification threshold for duplicates\n",
    "NOT_DUP_THRESHOLD = 0.1 #classification threshold for non-duplicates\n",
    "#Since the data is unbalanced, our mean prediction is around 0.16. So this is the reason of unbalanced thresholds\n",
    "\n",
    "MAX_UPDATE = 0.2 # maximum update on the dup probability (a high choice may ruin the predictions)\n",
    "DUP_UPPER_BOUND = 0.98 # do not update dup probabilities above this threshold\n",
    "NOT_DUP_LOWER_BOUND = 0.01 # do not update dup probabilities below this threshold\n",
    "# There is no significant gain between 0.98 and 1.00 for a dup \n",
    "# but there is significant loss if it is not really a dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(REPEAT):\n",
    "    dup_neighbors = defaultdict(set)\n",
    "\n",
    "    for dup, q1, q2 in zip(df_train[\"is_duplicate\"], df_train[\"question1\"], df_train[\"question2\"]): \n",
    "        if dup:\n",
    "            dup_neighbors[q1].add(q2)\n",
    "            dup_neighbors[q2].add(q1)\n",
    "    \n",
    "    for dup, q1, q2 in zip(test_label, df_test[\"question1\"], df_test[\"question2\"]): \n",
    "        if dup > DUP_THRESHOLD:\n",
    "            dup_neighbors[q1].add(q2)\n",
    "            dup_neighbors[q2].add(q1)\n",
    "\n",
    "    count = 0\n",
    "    for index, (q1, q2) in enumerate(zip(df_test[\"question1\"], df_test[\"question2\"])): \n",
    "        dup_neighbor_count = len(dup_neighbors[q1].intersection(dup_neighbors[q2]))\n",
    "        if dup_neighbor_count > 0 and test_label[index] < DUP_UPPER_BOUND:\n",
    "            update = min(MAX_UPDATE, (DUP_UPPER_BOUND - test_label[index])/2)\n",
    "            test_label[index] += update\n",
    "            count += 1\n",
    "\n",
    "    print(\"Edited:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(REPEAT):\n",
    "    not_dup_neighbors = defaultdict(set)\n",
    "\n",
    "    for dup, q1, q2 in zip(df_train[\"is_duplicate\"], df_train[\"question1\"], df_train[\"question2\"]): \n",
    "        if not dup:\n",
    "            not_dup_neighbors[q1].add(q2)\n",
    "            not_dup_neighbors[q2].add(q1)\n",
    "    \n",
    "    for dup, q1, q2 in zip(test_label, df_test[\"question1\"], df_test[\"question2\"]): \n",
    "        if dup < NOT_DUP_THRESHOLD:\n",
    "            not_dup_neighbors[q1].add(q2)\n",
    "            not_dup_neighbors[q2].add(q1)\n",
    "\n",
    "    count = 0\n",
    "    for index, (q1, q2) in enumerate(zip(df_test[\"question1\"], df_test[\"question2\"])): \n",
    "        dup_neighbor_count = len(not_dup_neighbors[q1].intersection(not_dup_neighbors[q2]))\n",
    "        if dup_neighbor_count > 0 and test_label[index] > NOT_DUP_LOWER_BOUND:\n",
    "            update = min(MAX_UPDATE, (test_label[index] - NOT_DUP_LOWER_BOUND)/2)\n",
    "            test_label[index] -= update\n",
    "            count += 1\n",
    "\n",
    "    print(\"Edited:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'test_id':df_test[\"test_id\"], 'is_duplicate':test_label})\n",
    "submission.to_csv('data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('data/simple_xgb2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "eli5.explain_weights_xgboost(bst, top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
